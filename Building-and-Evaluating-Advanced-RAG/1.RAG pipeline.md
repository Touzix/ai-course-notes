## Contexte g√©n√©ral : qu‚Äôest‚Äëce qu‚Äôun "Query Engine" en RAG ?

Le **RAG (Retrieval‚ÄëAugmented Generation)** est un syst√®me qui, √† partir d'une question de l‚Äôutilisateur, r√©cup√®re des extraits de documents pertinents, puis les envoie √† un LLM (comme GPT-3.5) pour g√©n√©rer une r√©ponse.

Un **query engine**, dans ce contexte, est un composant qui effectue deux √©tapes principales :

1. **R√©cup√©ration** (retrieval)‚ÄØ: retrouve des passages pertinents dans une base de documents.
2. **Synth√®se** (synthesis)‚ÄØ: envoie ces extraits + la question au LLM pour obtenir la r√©ponse.

Dans le cours, trois variantes de ce moteur sont pr√©sent√©es : **Direct Query Engine**, **Sentence Window Query Engine**, et **Auto‚ÄëMerging Query Engine**. Voyons comment ils diff√®rent.

---

## 1. **Direct Query Engine** (RAG de base)

C‚Äôest la version la plus simple. Le pipeline fonctionne ainsi :

1. Le document est **hach√© en chunks** (morceaux de taille fixe).
2. On indexe ces chunks dans une base de vecteurs.
3. √Ä une requ√™te, on r√©cup√®re les *K* chunks les plus proches (top‚ÄëK).
4. On envoie directement ces morceaux + la question √† l‚ÄôLLM.

\*\* Avantage\*\* : facile √† impl√©menter.
\*\* Limite\*\* : les chunks peuvent manquer de contexte, ou √™tre trop courts, et parfois la coh√©rence globale est faible.
([DeepLearning.AI - Learning Platform][1])

---

## 2. **Sentence Window Query Engine**

Ici, on am√©liore la gestion du contexte en deux √©tapes :

1. **Retrieval** sur des morceaux tr√®s petits : des phrases individuelles ou chunks minimalistes.
2. Lors de la synth√®se (synthesis), on **remplace** la phrase r√©cup√©r√©e par un **fen√™tre contextuelle** : la phrase plus les phrases autour (avant et apr√®s).

Cela donne un contexte plus riche au LLM sans perdre en pr√©cision √† l‚Äô√©tape de recherche.
([DeepLearning.AI - Learning Platform][2])

\*\* Avantages\*\* :

* Plus de contexte pour le LLM, donc g√©n√©ralement des r√©ponses plus pertinentes (meilleure *groundedness* et *context relevance*)‚ÄØ;
* Garde la pr√©cision du retrieval gr√¢ce √† l‚Äôembedding de petites phrases.

\*\* Attention\*\* : plus on agrandit la fen√™tre (par exemple 1 phrase ‚Üí 3 ‚Üí 5), plus le co√ªt (tokens, latence) augmente. Au-del√† d‚Äôune certaine taille, la qualit√© peut baisser car le LLM peut s‚Äôembrouiller.
([DeepLearning.AI - Learning Platform][2])

---

## 3. **Auto‚ÄëMerging Query Engine**

Cette approche utilise une **hi√©rarchie de chunks** :

* On d√©finit des **chunks enfants** (petits) et des **chunks parents** (plus larges, compos√©s des enfants).
* Pendant la requ√™te, si plusieurs enfants d‚Äôun m√™me parent sont r√©cup√©r√©s, on les **fusionne** automatiquement en un chunk parent : plus coh√©rent et moins fragment√©.

Exemple : quatre petits chunks enfants peuvent √™tre remplac√©s par un chunk parent plus long et fid√®le √† l‚Äôensemble.
([DeepLearning.AI - Learning Platform][3])

\*\* Avantages\*\* :

* Less fragmentation of information.
* Aids coherence and reduces redundant context.
* Maintains efficiency by substituting multiple pieces with one coherent piece.

---

## R√©sum√© comparatif

| Query Engine            | √âtape de retrieval                    | Contextualisation pour LLM           | Points forts                                |
| ----------------------- | ------------------------------------- | ------------------------------------ | ------------------------------------------- |
| **Direct Query Engine** | Chunks classiques                     | Aucun ajout de contexte              | Simple, rapide                              |
| **Sentence Window**     | Phrases (ou petits chunks)            | Fen√™tre autour de la phrase          | Contexte plus riche, meilleur soutien       |
| **Auto‚ÄëMerging**        | Chunks hi√©rarchiques (petits+parents) | Regroupement automatique des enfants | Moins de fragmentation, coh√©rence renforc√©e |

---

## En r√©sum√©, en mots simples :

* **Direct Query Engine** : tu r√©cup√®res des bouts de texte, tu les envoies √† l‚ÄôLLM. Rapide mais parfois trop d√©cousu.
* **Sentence Window** : tu rep√®res une phrase cl√©, et tu la compl√®tes avec ce qui la pr√©c√®de et suit, pour plus de sens.
* **Auto‚ÄëMerging** : si plusieurs morceaux importants appartiennent √† un m√™me gros bloc, tu les remplaces par ce gros bloc, plus coh√©rent.

## Exemple:
Voici une **m√™me question utilisateur** appliqu√©e √† un **m√™me document**, mais trait√©e par trois moteurs de requ√™te diff√©rents (**Direct**, **Sentence Window**, **Auto-Merging**) pour te montrer clairement leur impact.

---

### üìÑ **Document source** :

> *Albert Einstein est n√© en 1879 √† Ulm, en Allemagne. Il a publi√© la th√©orie de la relativit√© restreinte en 1905, suivie de la relativit√© g√©n√©rale en 1915. Il a re√ßu le prix Nobel de physique en 1921 pour sa d√©couverte de l'effet photo√©lectrique. Il a pass√© la derni√®re partie de sa vie √† Princeton, aux √âtats-Unis.*

---

### ‚ùì **Question utilisateur** :

> *Quand Einstein a-t-il re√ßu le prix Nobel, et pour quelle d√©couverte ?*

---

## üîπ 1. **Direct Query Engine**

**Retrieval** : r√©cup√®re uniquement ce chunk :

> *Il a re√ßu le prix Nobel de physique en 1921 pour sa d√©couverte de l'effet photo√©lectrique.*

**R√©ponse g√©n√©r√©e** :

> *Einstein a re√ßu le prix Nobel de physique en 1921 pour sa d√©couverte de l'effet photo√©lectrique.*

‚úÖ **Bonne r√©ponse**, car le chunk contient directement l'info.

üîª **Mais** : si la phrase √©tait partag√©e entre deux chunks (par ex. "Il a re√ßu le prix Nobel de physique en 1921." et "C'√©tait pour sa d√©couverte de l'effet photo√©lectrique."), la r√©ponse serait incompl√®te.

---

## üîπ 2. **Sentence Window Query Engine**

**Retrieval** : identifie la phrase cl√© :

> *Il a re√ßu le prix Nobel de physique en 1921 pour sa d√©couverte de l'effet photo√©lectrique.*

Puis √©largit le contexte :

> *Il a publi√© la th√©orie de la relativit√©...*
> *Il a re√ßu le prix Nobel...*
> *Il a pass√© la derni√®re partie de sa vie...*

**R√©ponse g√©n√©r√©e** :

> *Einstein a re√ßu le prix Nobel en 1921 pour sa d√©couverte de l'effet photo√©lectrique. Cela s'est produit apr√®s ses travaux sur la relativit√© restreinte et avant qu'il ne parte vivre √† Princeton.*

‚úÖ **Meilleure r√©ponse**, car elle situe l‚Äô√©v√©nement dans un contexte chronologique, ce qui peut aider pour d'autres questions ou r√©sum√©s.

---

## üîπ 3. **Auto-Merging Query Engine**

**Retrieval** : plusieurs phrases cl√©s sont proches ‚Üí elles sont fusionn√©es automatiquement en un chunk plus long :

> *Albert Einstein est n√© en 1879... Il a publi√© la th√©orie... re√ßu le prix Nobel... √† Princeton...*

**R√©ponse g√©n√©r√©e** :

> *Einstein a re√ßu le prix Nobel de physique en 1921 pour sa d√©couverte de l'effet photo√©lectrique. Il avait d√©j√† publi√© la relativit√© restreinte en 1905, et g√©n√©rale en 1915. Il termina sa carri√®re √† Princeton.*

‚úÖ R√©ponse **riche et fluide**, bonne coh√©rence, pas de r√©p√©tition de morceaux.

---

## üß† Conclusion p√©dagogique :

* **Direct** ‚Üí va droit au but, mais parfois trop "sec".
* **Sentence Window** ‚Üí offre du contexte cibl√©.
* **Auto-Merging** ‚Üí donne une vision d'ensemble, parfaite pour des r√©ponses narratives ou des r√©sum√©s.
